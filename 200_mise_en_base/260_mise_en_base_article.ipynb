{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "import parse_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dossier_data = '/home/michel/loi/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charge les infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(dbname='loi', user='loi', password='baba')\n",
    "curseur = connection.cursor()\n",
    "\n",
    "curseur.execute(\"select base_origine, categorie, cid, id_ from article where valide = True;\")\n",
    "liste_article = curseur.fetchall()\n",
    "\n",
    "curseur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n"
     ]
    }
   ],
   "source": [
    "valeurs_par_id_ = {}\n",
    "\n",
    "i = 0\n",
    "for base_origine, categorie, cid, id_ in liste_article:\n",
    "    nom_fichier = dossier_data + 'parse_article/' + id_\n",
    "\n",
    "    with open(nom_fichier, 'rb') as f:\n",
    "        valeurs = pickle.load(f)\n",
    "        \n",
    "        valeurs_par_id_[id_] = valeurs\n",
    "        \n",
    "        i += 1\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitements\n",
    "\n",
    "Cr√©ation de la nouvelle table contenant les articles :\n",
    "```\n",
    "create table article_2 (\n",
    "    base_origine varchar(21),\n",
    "    categorie varchar(21),\n",
    "    cid varchar(21),\n",
    "    id_ varchar(21),\n",
    "    valide boolean,\n",
    "\n",
    "    ID_ELI text,\n",
    "    ID_ELI_ALIAS text,\n",
    "    ANCIEN_ID text,\n",
    "    ORIGINE text,\n",
    "    URL text,\n",
    "    NATURE text,\n",
    "    NUM text,\n",
    "    MCS_ART text,\n",
    "    ETAT text,\n",
    "    DATE_DEBUT text,\n",
    "    DATE_FIN text,\n",
    "    TYPE text,\n",
    "    TEXTE text,\n",
    "    TITRES_TXT text,\n",
    "    TMS text,\n",
    "    VERSIONS text,\n",
    "    NOTA text,\n",
    "    SM text,\n",
    "    BLOC_TEXTUEL text,\n",
    "    LIENS text\n",
    ");\n",
    "create index on article_2 (id_);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serialisable(objet):\n",
    "    if isinstance(objet, parse_article.FrozenDict):\n",
    "        return {c: serialisable(v) for c, v in objet.items()}\n",
    "    elif isinstance(objet, frozenset):\n",
    "        return [serialisable(e) for e in objet]\n",
    "    elif isinstance(objet, tuple):\n",
    "        return [serialisable(e) for e in objet]\n",
    "    else:\n",
    "        return objet\n",
    "\n",
    "\n",
    "def traite_id_(base_origine, categorie, cid, id_):\n",
    "    valeurs = valeurs_par_id_[id_]\n",
    "    assert valeurs['ID'] == id_\n",
    "\n",
    "    valeurs_serialisables = {\n",
    "        'base_origine': base_origine,\n",
    "        'categorie': categorie,\n",
    "        'cid': cid,\n",
    "        'id_': id_,\n",
    "        'valide': True,\n",
    "\n",
    "        'ID_ELI': valeurs['ID_ELI'],\n",
    "        'ID_ELI_ALIAS': valeurs['ID_ELI_ALIAS'],\n",
    "        'ANCIEN_ID': valeurs['ANCIEN_ID'],\n",
    "        'ORIGINE': valeurs['ORIGINE'],\n",
    "        'URL': valeurs['URL'],\n",
    "        'NATURE': valeurs['NATURE'],\n",
    "        'NUM': valeurs['NUM'],\n",
    "        'MCS_ART': valeurs['MCS_ART'],\n",
    "        'ETAT': valeurs['ETAT'],\n",
    "        'DATE_DEBUT': valeurs['DATE_DEBUT'],\n",
    "        'DATE_FIN': valeurs['DATE_FIN'],\n",
    "        'TYPE': valeurs['TYPE'],\n",
    "        'NOTA': valeurs['NOTA'],\n",
    "        'SM': valeurs['SM'],\n",
    "        'BLOC_TEXTUEL': valeurs['BLOC_TEXTUEL'],\n",
    "    }\n",
    "\n",
    "    valeurs_serialisables['TEXTE'] = json.dumps(dict(valeurs['TEXTE']))\n",
    "    valeurs_serialisables['TITRES_TXT'] = json.dumps([dict(fd) for fd in valeurs['TITRES_TXT']])\n",
    "    valeurs_serialisables['VERSIONS'] = json.dumps([dict(fd) for fd in valeurs['VERSIONS']])\n",
    "    valeurs_serialisables['LIENS'] = json.dumps([dict(fd) for fd in valeurs['LIENS']])\n",
    "    valeurs_serialisables['TMS'] = json.dumps(serialisable(valeurs['TMS']))\n",
    "\n",
    "    return valeurs_serialisables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(dbname='loi', user='loi', password='baba')\n",
    "curseur = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valeurs = []\n",
    "for base_origine, categorie, cid, id_ in liste_article:\n",
    "    valeurs.append(traite_id_(base_origine, categorie, cid, id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liste_champ = list(valeurs[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liste_chaine = [\n",
    "    curseur.mogrify(\"(\" + ','.join([\"%s\"] * len(liste_champ)) + \")\",\n",
    "            [v[c] for c in liste_champ]\n",
    "        )\n",
    "    for v in valeurs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_fournee = 1000\n",
    "for i_fournee in range(len(liste_chaine)//nb_fournee + 1):\n",
    "    sous_liste = liste_chaine[i_fournee*nb_fournee:(i_fournee+1)*nb_fournee]\n",
    "    chaine_valeurs = b','.join(sous_liste)\n",
    "    \n",
    "    curseur.execute(b\"insert into article_2 (\" + b','.join([bytearray(c, 'utf-8') for c in liste_champ]) + b\") values \" + chaine_valeurs + b\";\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curseur.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
